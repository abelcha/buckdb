const e="import * as t from '.buck/types'\nimport { DuckdbCon } from '@buckdb/core'\nimport { GField, MetaModel, MS, SelectModel, VTypes } from './build.types'\nimport { formalize } from './formalise'\nimport { NestedKeyOf } from './generic-utils'\n\ntype FileFormats = 'parquet' | 'csv' | 'json' | 'arrow' | 'jsonl'\ntype CompressionType = 'auto' | 'none' | 'gzip' | 'zstd' | 'snappy' | 'brotli' | 'lz4'\ntype compressExtension = '.gz' | '.zst' | '.brotli' | '.lz4' | ''\ntype csvFilename = `${string}.csv${compressExtension}`\ntype parquetFilename = `${string}.parquet`\ntype jsonFilename = `${string}${'.json' | '.jsonl' | '.ndjson'}${compressExtension}`\n\nexport interface CsvCopyOptions {\n    /** The compression type for the file. By default this will be detected automatically from the file extension (e.g., `file.csv.gz` will use `gzip`, `file.csv.zst` will use `zstd`, and `file.csv` will use `none`). Options are `none`, `gzip`, `zstd`. @default 'auto' */\n    compression?: CompressionType\n    /** Specifies the date format to use when writing dates. @see https://duckdb.org/docs/sql/functions/dateformat */\n    dateformat?: string\n    /** The character that is written to separate columns within each row. @default ',' */\n    delim?: string\n    /** Alias for DELIM. The character that is written to separate columns within each row. @default ',' */\n    sep?: string\n    /** The character that should appear before a character that matches the `quote` value. @default '\"' */\n    escape?: string\n    /** The list of columns to always add quotes to, even if not required. @default [] */\n    force_quote?: string[]\n    /** Whether or not to write a header for the CSV file. @default true */\n    header?: boolean\n    /** The string that is written to represent a `NULL` value. @default '' (empty string) */\n    nullstr?: string\n    /** Prefixes the CSV file with a specified string. This option must be used in conjunction with `SUFFIX` and requires `HEADER` to be set to `false`. @default '' (empty string) */\n    prefix?: string\n    /** Appends a specified string as a suffix to the CSV file. This option must be used in conjunction with `PREFIX` and requires `HEADER` to be set to `false`. @default '' (empty string) */\n    suffix?: string\n    /** The quoting character to be used when a data value is quoted. @default '\"' */\n    quote?: string\n    /** Specifies the date format to use when writing timestamps. @see https://duckdb.org/docs/sql/functions/dateformat */\n    timestampformat?: string\n}\n/** Options for writing Parquet files using the COPY statement. @see https://duckdb.org/docs/sql/statements/copy#parquet-options */\nexport interface ParquetCopyOptions {\n    /** The compression format to use. @default 'snappy' */\n    compression?: CompressionType\n    /** Compression level, set between 1 (lowest compression, fastest) and 22 (highest compression, slowest). Only supported for zstd compression. @default 3 */\n    compression_level?: number\n    /** The `field_id` for each column. Pass `auto` to attempt to infer automatically. */\n    field_ids?: 'auto' | Record<string, any> // Type might need refinement based on actual usage\n    /** The target size of each row group in bytes. You can pass either a human-readable string, e.g., `2MB`, or an integer, i.e., the number of bytes. This option is only used when you have issued `SET preserve_insertion_order = false;`, otherwise, it is ignored. @default row_group_size * 1024 */\n    row_group_size_bytes?: number | string\n    /** The target size, i.e., number of rows, of each row group. @default 122880 */\n    row_group_size?: number\n    /** Create a new Parquet file if the current one has a specified number of row groups. If multiple threads are active, the number of row groups in a file may slightly exceed the specified number of row groups to limit the amount of locking â€“ similarly to the behaviour of `FILE_SIZE_BYTES`. However, if `per_thread_output` is set, only one thread writes to each file, and it becomes accurate again. */\n    row_groups_per_file?: number\n}\n/** Options for writing JSON files using the COPY statement. @see https://duckdb.org/docs/sql/statements/copy#json-options */\nexport interface JsonCopyOptions {\n    /** Whether to write a JSON array. If `true`, a JSON array of records is written, if `false`, newline-delimited JSON is written. @default false */\n    array?: boolean\n    /** The compression type for the file. By default this will be detected automatically from the file extension (e.g., `file.json.gz` will use `gzip`, `file.json.zst` will use `zstd`, and `file.json` will use `none`). Options are `none`, `gzip`, `zstd`. @default 'auto' */\n    compression?: CompressionType\n    /** Specifies the date format to use when writing dates. @see https://duckdb.org/docs/sql/functions/dateformat */\n    dateformat?: string\n    /** Specifies the date format to use when writing timestamps. @see https://duckdb.org/docs/sql/functions/dateformat */\n    timestampformat?: string\n}\n/** Generic options that might apply to multiple file formats or serve as a base. */\nexport interface GenericCopyOptions<A extends MetaModel = MetaModel, S extends SelectModel = {}, G = NestedKeyOf<A> | NestedKeyOf<S>> {\n    /** Explicit format specification, useful if filename doesn't have standard extension. */\n    format?: FileFormats  // Add other formats as needed\n    /** Compression setting, potentially overriding format-specific defaults or auto-detection. */\n    compression?: CompressionType\n    /** Whether or not to write to a temporary file first if the original file exists (target.csv.tmp). This prevents overwriting an existing file with a broken file in case the writing is cancelled. @default 'auto' */\n    use_tmp_file?: boolean | 'auto'\n    /** Whether or not to allow overwriting files if they already exist. Only has an effect when used with partition_by. @default false */\n    overwrite_or_ignore?: boolean\n    /** When set, all existing files inside targeted directories will be removed (not supported on remote filesystems). Only has an effect when used with partition_by. @default false */\n    overwrite?: boolean\n    /** When set, in the event a filename pattern is generated that already exists, the path will be regenerated to ensure no existing files are overwritten. Only has an effect when used with partition_by. @default false */\n    append?: boolean\n    /** Set a pattern to use for the filename, can optionally contain {uuid} to be filled in with a generated UUID or {id} which is replaced by an incrementing index. Only has an effect when used with partition_by. @default 'auto' */\n    filename_pattern?: string\n    /** Set the file extension that should be assigned to the generated file(s). @default 'auto' */\n    file_extension?: 'auto' | csvFilename | parquetFilename | jsonFilename\n    /** Generate one file per thread, rather than one file in total. This allows for faster parallel writing. @default false */\n    per_thread_output?: boolean\n    /** If this parameter is set, the COPY process creates a directory which will contain the exported files. If a file exceeds the set limit (specified as bytes such as 1000 or in human-readable format such as 1k), the process creates a new file in the directory. This parameter works in combination with PER_THREAD_OUTPUT. Note that the size is used as an approximation, and files can be occasionally slightly over the limit. @default '' (empty string) */\n    file_size_bytes?: string | number\n    /** The columns to partition by using a Hive partitioning scheme, see the partitioned writes section. @default [] (empty array) */\n    partition_by?: G | G[]\n    /** Whether or not to include the created filepath(s) (as a Files VARCHAR[] column) in the query result. @default false */\n    return_files?: boolean\n    /** Whether or not to write partition columns into files. Only has an effect when used with partition_by. @default false */\n    write_partition_columns?: boolean\n    [key: string]: any // Allow additional options for flexibility]\n}\ntype ReturnValue = { execute: () => Promise<any>; show: () => any; toSql: (opts?: any) => string }\n// Renamed interface to avoid naming conflict with method\nexport interface CopyToInterface<A extends MetaModel, S extends SelectModel = {}, Options extends GenericCopyOptions<A, S> = GenericCopyOptions<A, S>> {\n    to:\n    & ((destination: csvFilename, options?: CsvCopyOptions & Options) => ReturnValue)\n    & ((destination: parquetFilename, options?: ParquetCopyOptions & Options) => ReturnValue)\n    & ((destination: jsonFilename, options?: JsonCopyOptions & Options) => ReturnValue)\n    & ((destination: Exclude<string, csvFilename | parquetFilename | jsonFilename>, options?: Options) => ReturnValue);\n}\n\n\ndeclare function _copy<V extends VTypes, A extends MetaModel, S extends SelectModel = {}, SV = [], SS extends GField = t.DAnyField>(\n    source: MS<V, t.DMetaField, A, S, SV> | string\n): CopyToInterface<A, S>; // Changed to use the renamed interface\n\nfunction xcopy(\n    source: { toSql: () => string },\n) {\n    return {\n        to: (destination: string, options: Record<string, any> = {}) => {\n            // Get the SQL from the source\n            const sourceSql = typeof source === 'string' ? source : source.toSql()\n\n            // Build options string\n            const optionsArray: string[] = []\n\n            // Handle specific options first for consistent order (optional but good for testing)\n            if (options.format) {\n                // Use uppercase format value without quotes\n                optionsArray.push(`FORMAT ${options.format.toUpperCase()}`)\n            }\n            if (options.compression) {\n                // Use uppercase compression value without quotes\n                optionsArray.push(`COMPRESSION ${options.compression.toUpperCase()}`)\n            }\n\n            // Add any other options, handling types correctly\n            Object.entries(options).toSorted().forEach(([key, value]) => {\n                // Skip options already handled\n                if (key === 'partition_by') {\n                    optionsArray.push(`PARTITION_BY ${formalize(value)}`)\n                    return\n                }\n                if (key === 'format' || key === 'compression') {\n                    return\n                }\n\n                const keyUpper = key.toUpperCase()\n                if (Array.isArray(value)) {\n                    optionsArray.push(`${keyUpper} [${value.map(v => `'${v}'`).join(', ')}]`)\n                } else if (typeof value === 'string') {\n                    // Quote string values\n                    optionsArray.push(`${keyUpper} '${value}'`)\n                } else if (typeof value === 'boolean') {\n                    // Use uppercase TRUE/FALSE for booleans\n                    optionsArray.push(`${keyUpper} ${value ? 'TRUE' : 'FALSE'}`)\n                } else if (typeof value === 'number') {\n                    // Numbers are used directly\n                    optionsArray.push(`${keyUpper} ${value}`)\n                }\n                // Add handling for other types if necessary\n            })\n\n            // Build the COPY statement\n            const optionsStr = optionsArray.length > 0 ? `(${optionsArray.join(', ')})` : ''\n            const copyStatement = `COPY (${sourceSql}) TO '${destination}' ${optionsStr}`\n            return {\n                toSql: (opts?: Record<string, any>) => opts?.trim ? copyStatement.replaceAll(/(\\n|\\s)+/g, ' ') : copyStatement,\n                execute: async () => {\n                    // console.log('Executing COPY statement:', copyStatement)\n                    // Execute the COPY statement\n                    // Access the DuckDB connection (assuming the structure)\n                    const ddb = (source as any).toState?.().ddb as DuckdbCon\n                        || (source as any).ddb as DuckdbCon // Fallback if toState doesn't exist or doesn't have ddb\n\n                    if (!ddb || typeof ddb.query !== 'function') { // Check if ddb and ddb.query are valid\n                        throw new Error('Could not access DuckDB connection or query method from source')\n                    }\n\n                    return await ddb.query(copyStatement) // Use query method\n                }\n            }\n            // // Execute the COPY statement\n            // // Access the DuckDB connection (assuming the structure)\n            // const ddb = (source as any).toState?.().ddb as DuckdbCon\n            //     || (source as any).ddb as DuckdbCon // Fallback if toState doesn't exist or doesn't have ddb\n\n            // if (!ddb || typeof ddb.query !== 'function') { // Check if ddb and ddb.query are valid\n            //     throw new Error('Could not access DuckDB connection or query method from source')\n            // }\n\n            // await ddb.query(copyStatement) // Use query method\n        },\n    }\n}\nexport const copy = xcopy as unknown as typeof _copy\n\n// await from('duckdb_functions()').select().copyTo('sdq.parquet', {\n//     // compression: 'brotli'\n// }).execute()\n// const resp = await copy(\n//     from('duckdb_functions()').select()\n// ).to('partx', {\n//     partition_by: ['function_type'],\n//     format: 'parquet',\n//     overwrite: true,\n// })\n\n// console.log(resp.toSql())\n// console.log(await resp.execute())\n";export{e as default};
