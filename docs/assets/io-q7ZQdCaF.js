const e="// No specific imports needed from typedef for these declarations anymore\nimport { __serialize, FilterKeys, SerializeOrdered, SerializeValue } from './src/serializer.ts'\ntype CompressionOptions = 'none' | 'gzip' | 'zstd' | 'lz4' | 'lz4_raw' | 'brotli' | 'auto'\n// const __serialize = () =>\n/** Options for the read-csv function, maintaining original camelCase naming and order. */\ntype ReadCsvOptions = {\n    /** Skip type detection and assume all columns are of type `VARCHAR`. @default false */\n    all_varchar?: boolean // Corresponds to all_varchar\n    /** Allow the conversion of quoted values to `NULL` values. @default true */\n    allow_quoted_nulls?: boolean // Corresponds to allow_quoted_nulls\n    /** Auto detect CSV parameters. @see {@link https://duckdb.org/docs/stable/data/csv/auto_detection} @default true */\n    auto_detect?: boolean // Corresponds to auto_detect\n    /** Types that the sniffer uses when detecting column types. The `VARCHAR` type is always included as a fallback option. @see {@link https://duckdb.org/docs/stable/data/csv/auto_detection#auto_type_candidates-details} @default ['VARCHAR', 'BIGINT', 'DOUBLE', 'DATE', 'TIME', 'TIMESTAMP'] */\n    auto_type_candidates?: string[] // Corresponds to auto_type_candidates\n    /** Size of the buffers used to read files, in bytes. Must be large enough to hold four lines and can significantly impact performance. @default 16 * max_line_size */\n    buffer_size?: number // Corresponds to buffer_size\n    /** Alias for `names`. Column names, as a list. @see {@link https://duckdb.org/docs/stable/data/csv/tips#provide-names-if-the-file-does-not-contain-a-header} @default [] */\n    column_names?: string[] // Corresponds to column_names (alias of names)\n    /** Alias for `types`. Column types, as either a list (by position) or a struct (by name). @see {@link https://duckdb.org/docs/stable/data/csv/tips#override-the-types-of-specific-columns} @default {} or [] */\n    column_types?: string[] | Record<string, string> // Corresponds to column_types (alias of types)\n    /** Column names and types, as a struct (e.g., `{'col1': 'INTEGER', 'col2': 'VARCHAR'}`). Using this option disables auto detection. @default {} */\n    columns?: Record<string, string> // Corresponds to columns\n    /** Character used to initiate comments. Lines starting with a comment character (optionally preceded by space characters) are completely ignored; other lines containing a comment character are parsed only up to that point. @default '' */\n    comment?: string // Corresponds to comment\n    /** Method used to compress CSV files. By default this is detected automatically from the file extension (e.g., `t.csv.gz` will use gzip, `t.csv` will use `none`). @default 'auto' */\n    compression?: CompressionOptions // Corresponds to compression\n    /** Date format used when parsing and writing dates. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default '' */\n    dateformat?: string // Corresponds to dateformat\n    /** Decimal separator for numbers. @default '.' */\n    decimal_separator?: string // Corresponds to decimal_separator\n    /** Delimiter character used to separate columns within each line, e.g., `,` `;` `\\t`. The delimiter character can be up to 4 bytes, e.g., ðŸ¦†. Alias for `sep`. @default ',' */\n    delim?: string // Corresponds to delim\n    /** Alias for `types`. Column types, as either a list (by position) or a struct (by name). @see {@link https://duckdb.org/docs/stable/data/csv/tips#override-the-types-of-specific-columns} @default {} or [] */\n    dtypes?: string[] | Record<string, string> // Corresponds to dtypes (alias of types)\n    /** Encoding used by the CSV file. Options are `utf-8`, `utf-16`, `latin-1`. Not available in the `COPY` statement (which always uses `utf-8`). @default 'utf-8' */\n    encoding?: 'utf-8' | 'utf-16' | 'latin-1' // Corresponds to encoding\n    /** String used to escape the `quote` character within quoted values. @default '\"' */\n    escape?: string // Corresponds to escape\n    /** Add path of the containing file to each row, as a string column named `filename`. Relative or absolute paths are returned depending on the path or glob pattern provided to `read-csv`, not just filenames. @default false */\n    filename?: boolean // Corresponds to filename\n    force_match?: boolean // Not found in provided table description\n    /** Do not match values in the specified columns against the `NULL` string. In the default case where the `NULL` string is empty, this means that empty values are read as zero-length strings instead of `NULL`s. @default [] */\n    force_not_null?: string[] // Corresponds to force_not_null\n    /** First line of each file contains the column names. @default false */\n    header?: boolean // Corresponds to header\n    /** Interpret the path as a [Hive partitioned path](https://duckdb.org/docs/stable/data/partitioning/hive_partitioning). @default false */\n    hive_partitioning?: boolean // Corresponds to hive_partitioning\n    // hiveTypes?: any; // Not found in provided table description\n    // hiveTypesAutocast?: boolean; // Not found in provided table description\n    /** Ignore any parsing errors encountered. @default false */\n    ignore_errors?: boolean // Corresponds to ignore_errors\n    /** Maximum line size, in bytes. Not available in the `COPY` statement. Alias: `maximumLineSize`. @default 2000000 */\n    max_line_size?: number // Corresponds to max_line_size\n    /** Alias for `maxLineSize`. Maximum line size, in bytes. Not available in the `COPY` statement. @default 2000000 */\n    maximum_line_size?: number // Corresponds to maximum_line_size\n    /** Column names, as a list. Alias: `columnNames`. @see {@link https://duckdb.org/docs/stable/data/csv/tips#provide-names-if-the-file-does-not-contain-a-header} @default [] */\n    names?: string[] // Corresponds to names\n    /** New line character(s). Options are `'\\r'`,`'\\n'`, or `'\\r\\n'`. The CSV parser only distinguishes between single-character and double-character line delimiters. Therefore, it does not differentiate between `'\\r'` and `'\\n'`. @default '' // Empty string likely means auto-detect or system default */\n    new_line?: '\\r' | '\\n' | '\\r\\n' | '' // Corresponds to new_line\n    /** Normalize column names. This removes any non-alphanumeric characters from them. Column names that are reserved SQL keywords are prefixed with an underscore character (`_`). @default false */\n    normalize_names?: boolean // Corresponds to normalize_names\n    /** Pad the remaining columns on the right with `NULL` values when a line lacks columns. @default false */\n    null_padding?: boolean // Corresponds to null_padding\n    /** Strings that represent a `NULL` value. Alias: `null`. @default '' */\n    nullstr?: string | string[] // Corresponds to nullstr\n    /** Use the parallel CSV reader. @default true */\n    parallel?: boolean // Corresponds to parallel\n    /** String used to quote values. @default '\"' */\n    quote?: string // Corresponds to quote\n    /** Upper limit on the number of faulty lines per file that are recorded in the rejects table. Setting this to `0` means that no limit is applied. @default 0 */\n    rejects_limit?: number // Corresponds to rejects_limit\n    /** Name of the [temporary table where information on faulty scans is stored](https://duckdb.org/docs/stable/data/csv/reading_faulty_csv_files#reject-scans). @default 'reject_scans' */\n    rejects_scan?: string // Corresponds to rejects_scan\n    /** Name of the [temporary table where information on faulty lines is stored](https://duckdb.org/docs/stable/data/csv/reading_faulty_csv_files#reject-errors). @default 'reject_errors' */\n    rejects_table?: string // Corresponds to rejects_table\n    /** Number of sample lines for [auto detection of parameters](https://duckdb.org/docs/stable/data/csv/auto_detection). @default 20480 */\n    sample_size?: number // Corresponds to sample_size\n    /** Delimiter character used to separate columns within each line, e.g., `,` `;` `\\t`. The delimiter character can be up to 4 bytes, e.g., ðŸ¦†. Alias for `delim`. @default ',' */\n    sep?: string // Corresponds to sep\n    /** Number of lines to skip at the start of each file. @default 0 */\n    skip?: number // Corresponds to skip\n    /** Skip any lines with errors and store them in the rejects table. @default false */\n    store_rejects?: boolean // Corresponds to store_rejects\n    /** Enforces the strictness level of the CSV Reader. When set to `true`, the parser will throw an error upon encountering any issues. When set to `false`, the parser will attempt to read structurally incorrect files. Use with caution. @default true */\n    strict_mode?: boolean // Corresponds to strict_mode\n    /** [Timestamp format](https://duckdb.org/docs/stable/sql/functions/dateformat) used when parsing and writing timestamps. Alias: `timestamp_format` (COPY statement only). @default '' */\n    timestampformat?: string // Corresponds to timestampformat\n    /** Column types, as either a list (by position) or a struct (by name). Alias: `dtypes`, `columnTypes`. @see {@link https://duckdb.org/docs/stable/data/csv/tips#override-the-types-of-specific-columns} @default {} or [] */\n    types?: string[] | Record<string, string> // Corresponds to types\n    /** Align columns from different files [by column name](https://duckdb.org/docs/stable/data/multiple_files/combining_schemas#union-by-name) instead of position. Using this option increases memory consumption. @default false */\n    union_by_name?: boolean // Corresponds to union_by_name\n    // --- Aliases not explicitly listed in original opts but covered by descriptions ---\n    /** Alias for `dateformat`; only available in the `COPY` statement. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default '' */\n    date_format?: string\n    /** Alias for `delim`; only available in the `COPY` statement. @default ',' */\n    delimiter?: string\n    /** Alias for `nullstr`. @default '' */\n    null?: string | string[]\n    /** Alias for `timestampformat`; only available in the `COPY` statement. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default '' */\n    timestamp_format?: string\n} & {\n    // Include properties from original opts that were not in the table, marked as any\n    force_match?: boolean\n    hive_types?: any\n    hive_types_autocast?: boolean\n}\ntype CsvKeys = ['all_varchar', 'allow_quoted_nulls', 'auto_detect', 'auto_type_candidates', 'buffer_size', 'column_names', 'column_types', 'columns', 'comment', 'compression', 'dateformat', 'decimal_separator', 'delim', 'dtypes', 'encoding', 'escape', 'filename', 'force_match', 'force_not_null', 'header', 'hive_partitioning', 'ignore_errors', 'max_line_size', 'maximum_line_size', 'names', 'new_line', 'normalize_names', 'null_padding', 'nullstr', 'parallel', 'quote', 'rejects_limit', 'rejects_scan', 'rejects_table', 'sample_size', 'sep', 'skip', 'store_rejects', 'strict_mode', 'timestampformat', 'types', 'union_by_name', 'date_format', 'delimiter', 'timestamp_format', 'force_match', 'hive_types', 'hive_types_autocast']\n/**\n * Options for the read-json function, maintaining original camelCase naming and order from signature.\n */\ntype ReadJsonOptions = {\n    /** Whether to auto-detect the names of the keys and data types of the values automatically. @default true */\n    auto_detect?: boolean // Corresponds to auto_detect (BOOL)\n    /** A struct that specifies the key names and value types contained within the JSON file (e.g., `{key1: 'INTEGER', key2: 'VARCHAR'}`). If `auto_detect` is enabled these will be inferred. @default {} */\n    columns?: Record<string, string> // Corresponds to columns (STRUCT)\n    /** Compression method (e.g., 'gzip'). Not in table, using VARCHAR from signature. */\n    compression?: CompressionOptions // Not in table, using DVarcharable -> string\n    /** Convert strings to integers. Not in table, using BOOL from signature. */\n    convert_strings_to_integers?: boolean // Not in table, using DBoolable -> boolean\n    /** Alias for dateformat. Not in table, using VARCHAR from signature. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} */\n    date_format?: string // Not in table, using DVarcharable -> string\n    /** Specifies the date format to use when parsing dates. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default 'iso' */\n    dateformat?: string // Corresponds to dateformat (VARCHAR)\n    /** The JSON reader divides the number of appearances of each JSON field by the auto-detection sample size. If the average over the fields of an object is less than this threshold, it will default to using a `MAP` type with value type of merged field types. @default 0.1 */\n    field_appearance_threshold?: number // Corresponds to field_appearance_threshold (DOUBLE)\n    /** Add filename column. Not in table, using ANY | BIGINT from signature -> boolean | number | bigint. Assuming boolean based on read-csv. */\n    filename?: boolean // Not in table, guessing boolean based on read-csv\n    /** JSON format (e.g., 'auto', 'newline_delimited', 'array'). Not in table, using VARCHAR | BIGINT from signature -> string | number | bigint. Assuming string. */\n    format?: 'auto' | 'newline_delimited' | 'array' | string // Not in table, guessing string options\n    /** Hive partitioning. Not in table, using BOOL | VARCHAR from signature -> boolean | string. Assuming boolean based on read-csv. */\n    hive_partitioning?: boolean // Not in table, guessing boolean based on read-csv\n    /** Hive types. Not in table, using ANY | DOUBLE from signature -> any. */\n    hive_types?: any // Not in table, using any\n    /** Hive types autocast. Not in table, using BOOL | VARCHAR from signature -> boolean | string. Assuming boolean based on read-csv. */\n    hive_types_autocast?: boolean // Not in table, guessing boolean based on read-csv\n    /** Ignore errors. Not in table, using BOOL | UINTEGER from signature -> boolean | number. Assuming boolean based on read-csv. */\n    ignore_errors?: boolean // Not in table, guessing boolean based on read-csv\n    /** Controls the threshold for number of columns whose schema will be auto-detected; if JSON schema auto-detection would infer a `STRUCT` type for a field that has *more* than this threshold number of subfields, it infers a `MAP` type instead. Set to `-1` to disable `MAP` inference. @default 200 */\n    map_inference_threshold?: number // Corresponds to map_inference_threshold (BIGINT)\n    /** Maximum nesting depth to which the automatic schema detection detects types. Set to -1 to fully detect nested JSON types. @default -1 */\n    maximum_depth?: number // Corresponds to maximum_depth (BIGINT)\n    /** Maximum object size. Not in table, using UINTEGER | BOOL from signature -> number | boolean. Assuming number. */\n    maximum_object_size?: number // Not in table, guessing number\n    /** Maximum sample files. Not in table, using BIGINT from signature -> number. */\n    maximum_sample_files?: number // Not in table, using number\n    /** Can be one of `auto`, `true`, `false`. @default 'auto' */\n    records?: 'auto' | boolean // Corresponds to records (VARCHAR), mapping 'true'/'false' strings to boolean\n    /** Option to define number of sample objects for automatic JSON type detection. Set to -1 to scan the entire input file. @default 20480 */\n    sample_size?: number // Corresponds to sample_size (UBIGINT -> number | bigint)\n    /** Alias for timestampformat. Specifies the date format to use when parsing timestamps. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default 'iso' */\n    timestamp_format?: string // Alias for timestampformat (VARCHAR)\n    /** Specifies the date format to use when parsing timestamps. @see {@link https://duckdb.org/docs/stable/sql/functions/dateformat} @default 'iso' */\n    timestampformat?: string // Corresponds to timestampformat (VARCHAR)\n    /** Whether the schema's of multiple JSON files should be [unified](https://duckdb.org/docs/stable/data/multiple_files/combining_schemas). @default false */\n    union_by_name?: boolean // Corresponds to union_by_name (BOOL)\n}\ntype JsonReadKeys = ['auto_detect', 'columns', 'compression', 'convert_strings_to_integers', 'date_format', 'dateformat', 'field_appearance_threshold', 'filename', 'format', 'hive_partitioning', 'hive_types', 'hive_types_autocast', 'ignore_errors', 'map_inference_threshold', 'maximum_depth', 'maximum_object_size', 'maximum_sample_files', 'records', 'sample_size', 'timestamp_format', 'timestampformat', 'union_by_name']\n/**\n /** Options for the read-json_objects function, maintaining original camelCase naming and order from signature. */\ntype ReadJsonObjectsOptions = {\n    /** The compression type for the file. By default this will be detected automatically from the file extension (e.g., `t.json.gz` will use gzip, `t.json` will use none). Options are `none`, `gzip`, `zstd` and `auto_detect`. @default 'auto_detect' */\n    compression?: CompressionOptions // Corresponds to compression (VARCHAR)\n    /** Whether or not an extra `filename` column should be included in the result. @default false */\n    filename?: boolean // Corresponds to filename (BOOL)\n    /** Can be one of `auto`, `unstructured`, `newline_delimited` and `array`. @default 'array' */\n    format?: 'auto' | 'unstructured' | 'newline_delimited' | 'array' // Corresponds to format (VARCHAR)\n    /** Whether or not to interpret the path as a [Hive partitioned path](https://duckdb.org/docs/stable/data/partitioning/hive_partitioning). @default false */\n    hive_partitioning?: boolean // Corresponds to hive_partitioning (BOOL)\n    /** Hive types. Not in table, using ANY | BOOL from signature -> any. */\n    hive_types?: any // Not in table, using any\n    /** Hive types autocast. Not in table, using BOOL from signature -> boolean. */\n    hive_types_autocast?: boolean // Not in table, using boolean\n    /** Whether to ignore parse errors (only possible when `format` is `newline_delimited`). @default false */\n    ignore_errors?: boolean // Corresponds to ignore_errors (BOOL)\n    /** The maximum size of a JSON object (in bytes). @default 16777216 */\n    maximum_object_size?: number // Corresponds to maximum_object_size (UINTEGER -> number)\n    /** The maximum number of JSON files sampled for auto-detection. Not in signature, using BIGINT from table -> number. @default 32 */\n    maximum_sample_files?: number // Corresponds to maximum_sample_files (BIGINT)\n    /** Union by name. Not in table, using ANY | BOOL from signature -> boolean. */\n    union_by_name?: boolean // Not in table, using boolean\n}\ntype JsonReadObjectsKeys = ['compression', 'filename', 'format', 'hive_partitioning', 'hive_types', 'hive_types_autocast', 'ignore_errors', 'maximum_object_size', 'maximum_sample_files', 'union_by_name']\n/** Options for the read-parquet function, maintaining original camelCase naming and order from signature. */\ntype ReadParquetOptions = {\n    /** Parquet files generated by legacy writers do not correctly set the `UTF8` flag for strings, causing string columns to be loaded as `BLOB` instead. Set this to true to load binary columns as strings. @default false */\n    binary_as_string?: boolean // Corresponds to binary_as_string (BOOL)\n    /** Compression. Not in table, using VARCHAR | BOOL from signature -> string | boolean. Assuming string. */\n    compression?: CompressionOptions // Not in table, guessing string\n    /** Debug use OpenSSL. Not in table, using BOOL from signature -> boolean. */\n    debug_use_openssl?: boolean // Not in table, using boolean\n    /** Configuration for [Parquet encryption](https://duckdb.org/docs/stable/data/parquet/encryption). @default - (No default specified) */\n    encryption_config?: Record<string, any> // Corresponds to encryption_config (STRUCT)\n    /** Explicit cardinality. Not in table, using UBIGINT | ANY from signature -> number | any. Assuming any. */\n    explicit_cardinality?: any // Not in table, using any\n    /** Whether or not to include the `file_row_number` column. @default false */\n    file_row_number?: boolean // Corresponds to file_row_number (BOOL)\n    /** Whether or not an extra `filename` column should be included in the result. @default false */\n    filename?: boolean // Corresponds to filename (BOOL)\n    /** Whether or not to interpret the path as a [Hive partitioned path](https://duckdb.org/docs/stable/data/partitioning/hive_partitioning). @default true */\n    hive_partitioning?: boolean // Corresponds to hive_partitioning (BOOL)\n    /** Hive types. Not in table, using ANY | BOOL from signature -> any. */\n    hive_types?: any // Not in table, using any\n    /** Hive types autocast. Not in table, using BOOL | ANY from signature -> boolean | any. Assuming boolean. */\n    hive_types_autocast?: boolean // Not in table, guessing boolean\n    /** Parquet version. Not in table, using VARCHAR | BOOL from signature -> string | boolean. Assuming string. */\n    parquet_version?: string // Not in table, guessing string\n    /** Schema. Not in table, using ANY | BOOL from signature -> any. */\n    schema?: any // Not in table, using any\n    /** Whether the columns of multiple schemas should be [unified by name](https://duckdb.org/docs/stable/data/multiple_files/combining_schemas), rather than by position. @default false */\n    union_by_name?: boolean // Corresponds to union_by_name (BOOL)\n}\ntype ParquetKeys = ['binary_as_string', 'compression', 'debug_use_openssl', 'encryption_config', 'explicit_cardinality', 'file_row_number', 'filename', 'hive_partitioning', 'hive_types', 'hive_types_autocast', 'parquet_version', 'schema', 'union_by_name']\n/**\n * Options for the delta_scan function, inferred from signature.\n */\ntype DeltaScanOptions = {\n    binary_as_string?: boolean\n    compression?: CompressionOptions // VARCHAR | ANY -> string | any\n    debug_use_openssl?: boolean | any // BOOLEAN | ANY -> boolean | any\n    delta_file_number?: boolean\n    encryption_config?: any\n    explicit_cardinality?: boolean | number | bigint // UBIGINT | BOOLEAN -> number | boolean\n    file_row_number?: boolean\n    filename?: boolean | any // ANY | BOOLEAN -> boolean | any\n    hive_partitioning?: boolean | string // BOOLEAN | VARCHAR -> boolean | string\n    hive_types?: string | any // ANY | VARCHAR -> string | any\n    hive_types_autocast?: boolean\n    parquet_version?: string | number | bigint // VARCHAR | UBIGINT -> string | number | bigint\n    pushdown_filters?: string // VARCHAR -> string\n    pushdown_partition_info?: boolean\n    union_by_name?: boolean\n}\ntype DeltaScanKeys = ['binary_as_string', 'compression', 'debug_use_openssl', 'delta_file_number', 'encryption_config', 'explicit_cardinality', 'file_row_number', 'filename', 'hive_partitioning', 'hive_types', 'hive_types_autocast', 'parquet_version', 'pushdown_filters', 'pushdown_partition_info', 'union_by_name']\n/**\n * Options for the read-xlsx function, inferred from signature.\n */\ntype ReadXlsxOptions = {\n    all_varchar?: boolean\n    empty_as_varchar?: boolean\n    header?: boolean\n    ignore_errors?: boolean\n    normalize_names?: boolean\n    range?: string // VARCHAR -> string\n    sheet?: string // VARCHAR -> string\n    stop_at_empty?: boolean\n}\n/**\n * Options for the iceberg_metadata function.\n */\ntype IcebergMetadataOptions = {\n    allow_moved_paths?: boolean\n    metadata_compression_codec?: string\n    skip_schema_inference?: boolean\n    version_name_format?: string\n    version?: string\n}\n\ntype IcebergMetadataKeys = ['allow_moved_paths', 'metadata_compression_codec', 'skip_schema_inference', 'version_name_format', 'version']\n\n/**\n * Options for the iceberg_scan function.\n */\ntype IcebergScanOptions = {\n    version_name_format?: string\n    snapshot_from_timestamp?: Date | string\n    version?: string\n    metadata_compression_codec?: string\n    allow_moved_paths?: boolean\n    skip_schema_inference?: boolean\n    mode?: string\n    hive_types_autocast?: boolean\n    snapshot_from_id?: string\n    hive_partitioning?: boolean\n    hive_types?: boolean\n    compression?: CompressionOptions\n    explicit_cardinality?: number | bigint\n    union_by_name?: number | bigint\n    debug_use_openssl?: string\n    binary_as_string?: boolean\n    filename?: number | bigint\n    parquet_version?: string\n    file_row_number?: string\n    encryption_config?: boolean\n}\n\ntype IcebergScanKeys = ['version_name_format', 'snapshot_from_timestamp', 'version', 'metadata_compression_codec', 'allow_moved_paths', 'skip_schema_inference', 'mode', 'hive_types_autocast', 'snapshot_from_id', 'hive_partitioning', 'hive_types', 'compression', 'explicit_cardinality', 'union_by_name', 'debug_use_openssl', 'binary_as_string', 'filename', 'parquet_version', 'file_row_number', 'encryption_config']\n\n/**\n * Options for the iceberg_snapshots function.\n */\ntype IcebergSnapshotsOptions = {\n    skip_schema_inference?: boolean\n    version_name_format?: string\n    version?: string\n    metadata_compression_codec?: string\n}\n\ntype IcebergSnapshotsKeys = ['skip_schema_inference', 'version_name_format', 'version', 'metadata_compression_codec']\n\n/**\n * Options for the ST_Read function.\n * Read and import a variety of geospatial file formats using the GDAL library.\n */\ntype STReadOptions = {\n    /** If set, return geometries in a wkb_geometry column with type WKB_BLOB instead of GEOMETRY */\n    keep_wkb?: boolean\n    /** Maximum batch size for reading */\n    max_batch_size?: number\n    /** Scan through all layers sequentially and return first matching layer. Required for some drivers (e.g. OSM) */\n    sequential_layer_scan?: boolean\n    /** Layer name or index (0-based) to read. If NULL, first layer is returned */\n    layer?: string\n    /** List of sibling files required to open the file */\n    sibling_files?: string[]\n    /** WKB blob to filter rows that intersect with given geometry */\n    spatial_filter?: any\n    /** BOX_2D to filter rows that intersect with given bounding box */\n    spatial_filter_box?: any\n    /** List of allowed GDAL driver names. If empty, all drivers allowed */\n    allowed_drivers?: string[]\n    /** Key-value pairs passed to GDAL driver to control file opening */\n    open_options?: string[]\n}\n\ntype STReadKeys = ['keep_wkb', 'max_batch_size', 'sequential_layer_scan', 'layer', 'sibling_files', 'spatial_filter', 'spatial_filter_box', 'allowed_drivers', 'open_options']\n\n/**\n * Options for the ST_ReadSHP function.\n */\ntype STReadSHPOptions = {\n    /** Character encoding for DBF file */\n    encoding?: string\n}\n\ntype STReadSHPKeys = ['encoding']\n\ntype ReadXlsxKeys = ['all_varchar', 'empty_as_varchar', 'header', 'ignore_errors', 'normalize_names', 'range', 'sheet', 'stop_at_empty']\ntype CsvKeyOpts = StringArrayToUnion<CsvKeys>\ntype StringArrayToUnion<T extends readonly string[]> = T extends readonly [infer First, ...infer Rest extends readonly string[]] ? First & string | StringArrayToUnion<Rest>\n    : never\ntype Opt<T> = T extends Record<string, any> ? T : never\ntype RetCon<S extends string, K extends readonly string[], F, U extends Record<string, any>> = keyof U extends undefined ? `${S}(${SerializeValue<F>})`\n    : `${S}(${SerializeValue<F>},${SerializeOrdered<FilterKeys<K, U>, U>})`\ntype Fnx = {\n    read_csv<const F extends string, const U extends ReadCsvOptions>(args: F): RetCon<'read_csv', CsvKeys, [F], {}>\n    read_csv<const F extends readonly string[]>(...args: F[]): RetCon<'read_csv', CsvKeys, F, {}>\n    read_csv<const F extends string[], const U extends ReadCsvOptions>(...args: [...F, U]): RetCon<'read_csv', CsvKeys, F, U>\n    read_csv<const F extends readonly string[], const U extends ReadCsvOptions>(...args: [...F[], U]): RetCon<'read_csv', CsvKeys, F, U>\n    read_json<const F extends string, const U extends ReadJsonOptions>(args: F): RetCon<'read_json', CsvKeys, [F], {}>\n    read_json<const F extends readonly string[]>(...args: F[]): RetCon<'read_json', CsvKeys, F, {}>\n    read_json<const F extends string[], const U extends ReadJsonOptions>(...args: [...F, U]): RetCon<'read_json', CsvKeys, F, U>\n    read_json<const F extends readonly string[], const U extends ReadJsonOptions>(...args: [...F[], U]): RetCon<'read_json', CsvKeys, F, U>\n    read_json_objects<const F extends string, const U extends ReadJsonObjectsOptions>(args: F): RetCon<'read_json_objects', CsvKeys, [F], {}>\n    read_json_objects<const F extends readonly string[]>(...args: F[]): RetCon<'read_json_objects', CsvKeys, F, {}>\n    read_json_objects<const F extends string[], const U extends ReadJsonObjectsOptions>(...args: [...F, U]): RetCon<'read_json_objects', CsvKeys, F, U>\n    read_json_objects<const F extends readonly string[], const U extends ReadJsonObjectsOptions>(...args: [...F[], U]): RetCon<'read_json_objects', CsvKeys, F, U>\n    read_parquet<const F extends string, const U extends ReadParquetOptions>(args: F): RetCon<'read_parquet', CsvKeys, [F], {}>\n    read_parquet<const F extends readonly string[]>(...args: F[]): RetCon<'read_parquet', CsvKeys, F, {}>\n    read_parquet<const F extends string[], const U extends ReadParquetOptions>(...args: [...F, U]): RetCon<'read_parquet', CsvKeys, F, U>\n    read_parquet<const F extends readonly string[], const U extends ReadParquetOptions>(...args: [...F[], U]): RetCon<'read_parquet', CsvKeys, F, U>\n    delta_scan<const F extends string, const U extends DeltaScanOptions>(args: F): RetCon<'delta_scan', DeltaScanKeys, [F], {}>\n    delta_scan<const F extends readonly string[]>(...args: F[]): RetCon<'delta_scan', DeltaScanKeys, F, {}>\n    delta_scan<const F extends string[], const U extends DeltaScanOptions>(...args: [...F, U]): RetCon<'delta_scan', DeltaScanKeys, F, U>\n    delta_scan<const F extends readonly string[], const U extends DeltaScanOptions>(...args: [...F[], U]): RetCon<'delta_scan', DeltaScanKeys, F, U>\n    parquet_scan<const F extends string, const U extends ReadParquetOptions>(args: F): RetCon<'parquet_scan', CsvKeys, [F], {}>\n    parquet_scan<const F extends readonly string[]>(...args: F[]): RetCon<'parquet_scan', CsvKeys, F, {}>\n    parquet_scan<const F extends string[], const U extends ReadParquetOptions>(...args: [...F, U]): RetCon<'parquet_scan', CsvKeys, F, U>\n    parquet_scan<const F extends readonly string[], const U extends ReadParquetOptions>(...args: [...F[], U]): RetCon<'parquet_scan', CsvKeys, F, U>\n    read_xlsx<const F extends string, const U extends ReadXlsxKeys>(args: F): RetCon<'read_xlsx', CsvKeys, [F], {}>\n    read_xlsx<const F extends readonly string[]>(...args: F[]): RetCon<'read_xlsx', CsvKeys, F, {}>\n    read_xlsx<const F extends string[], const U extends ReadXlsxKeys>(...args: [...F, U]): RetCon<'read_xlsx', CsvKeys, F, U>\n    read_xlsx<const F extends readonly string[], const U extends ReadXlsxKeys>(...args: [...F[], U]): RetCon<'read_xlsx', CsvKeys, F, U>\n    read_text<const F extends string, const U extends {}>(args: F): RetCon<'read_text', [], [F], {}>\n    read_text<const F extends readonly string[]>(...args: F[]): RetCon<'read_text', [], F, {}>\n    read_text<const F extends string[], const U extends {}>(...args: [...F, U]): RetCon<'read_text', [], F, U>\n    read_text<const F extends readonly string[], const U extends {}>(...args: [...F[], U]): RetCon<'read_text', [], F, U>\n    iceberg_metadata<const F extends string, const U extends IcebergMetadataOptions>(args: F): RetCon<'iceberg_metadata', IcebergMetadataKeys, [F], {}>\n    iceberg_metadata<const F extends readonly string[]>(...args: F[]): RetCon<'iceberg_metadata', IcebergMetadataKeys, F, {}>\n    iceberg_metadata<const F extends string[], const U extends IcebergMetadataOptions>(...args: [...F, U]): RetCon<'iceberg_metadata', IcebergMetadataKeys, F, U>\n    iceberg_metadata<const F extends readonly string[], const U extends IcebergMetadataOptions>(...args: [...F[], U]): RetCon<'iceberg_metadata', IcebergMetadataKeys, F, U>\n    iceberg_scan<const F extends string, const U extends IcebergScanOptions>(args: F): RetCon<'iceberg_scan', IcebergScanKeys, [F], {}>\n    iceberg_scan<const F extends readonly string[]>(...args: F[]): RetCon<'iceberg_scan', IcebergScanKeys, F, {}>\n    iceberg_scan<const F extends string[], const U extends IcebergScanOptions>(...args: [...F, U]): RetCon<'iceberg_scan', IcebergScanKeys, F, U>\n    iceberg_scan<const F extends readonly string[], const U extends IcebergScanOptions>(...args: [...F[], U]): RetCon<'iceberg_scan', IcebergScanKeys, F, U>\n    iceberg_snapshots<const F extends string, const U extends IcebergSnapshotsOptions>(args: F): RetCon<'iceberg_snapshots', IcebergSnapshotsKeys, [F], {}>\n    iceberg_snapshots<const F extends readonly string[]>(...args: F[]): RetCon<'iceberg_snapshots', IcebergSnapshotsKeys, F, {}>\n    iceberg_snapshots<const F extends string[], const U extends IcebergSnapshotsOptions>(...args: [...F, U]): RetCon<'iceberg_snapshots', IcebergSnapshotsKeys, F, U>\n    iceberg_snapshots<const F extends readonly string[], const U extends IcebergSnapshotsOptions>(...args: [...F[], U]): RetCon<'iceberg_snapshots', IcebergSnapshotsKeys, F, U>\n    ST_Read<const F extends string, const U extends STReadOptions>(args: F): RetCon<'ST_Read', STReadKeys, [F], {}>\n    ST_Read<const F extends readonly string[]>(...args: F[]): RetCon<'ST_Read', STReadKeys, F, {}>\n    ST_Read<const F extends string[], const U extends STReadOptions>(...args: [...F, U]): RetCon<'ST_Read', STReadKeys, F, U>\n    ST_Read<const F extends readonly string[], const U extends STReadOptions>(...args: [...F[], U]): RetCon<'ST_Read', STReadKeys, F, U>\n    ST_Read_Meta<const F extends string>(args: F): RetCon<'ST_Read_Meta', [], [F], {}>\n    ST_Read_Meta<const F extends readonly string[]>(...args: F[]): RetCon<'ST_Read_Meta', [], F, {}>\n    ST_ReadOsm<const F extends string>(args: F): RetCon<'ST_ReadOsm', [], [F], {}>\n    ST_ReadOsm<const F extends readonly string[]>(...args: F[]): RetCon<'ST_ReadOsm', [], F, {}>\n    ST_ReadSHP<const F extends string, const U extends STReadSHPOptions>(args: F): RetCon<'ST_ReadSHP', STReadSHPKeys, [F], {}>\n    ST_ReadSHP<const F extends readonly string[]>(...args: F[]): RetCon<'ST_ReadSHP', STReadSHPKeys, F, {}>\n    ST_ReadSHP<const F extends string[], const U extends STReadSHPOptions>(...args: [...F, U]): RetCon<'ST_ReadSHP', STReadSHPKeys, F, U>\n    ST_ReadSHP<const F extends readonly string[], const U extends STReadSHPOptions>(...args: [...F[], U]): RetCon<'ST_ReadSHP', STReadSHPKeys, F, U>\n}\n/** Helper function to serialize function calls with optional options object. */\nconst fnSerial = (name = '', args: any[]) => {\n    let opts = {}\n    const last = args[args.length - 1]\n    if (typeof last === 'object' && last !== null && !Array.isArray(last)) {\n        opts = args.pop() || {}\n    }\n    if (!Object.keys(opts).length) {\n        return `${name}([${args.map((e = '') => `'${e}'`)}])`\n    }\n    return `${name}([${args.map((e = '') => `'${e}'`)}],${__serialize(opts)})`\n}\n/** Collection of functions for reading various file formats, serialized for query building. */\nconst Fncx = {\n    read_csv: (...args: any) => fnSerial('read_csv', args),\n    read_json: (...args: any) => fnSerial('read_json', args),\n    read_json_objects: (...args: any) => fnSerial('read_json_objects', args),\n    read_parquet: (...args: any) => fnSerial('read_parquet', args),\n    delta_scan: (...args: any) => fnSerial('delta_scan', args),\n    parquet_scan: (...args: any) => fnSerial('parquet_scan', args),\n    read_xlsx: (...args: any) => fnSerial('read_xlsx', args),\n    read_text: (...args: any) => fnSerial('read_text', args),\n    iceberg_metadata: (...args: any) => fnSerial('iceberg_metadata', args),\n    iceberg_scan: (...args: any) => fnSerial('iceberg_scan', args),\n    iceberg_snapshots: (...args: any) => fnSerial('iceberg_snapshots', args),\n    ST_Read: (...args: any) => fnSerial('ST_Read', args),\n    ST_Read_Meta: (...args: any) => fnSerial('ST_Read_Meta', args),\n    ST_ReadOsm: (...args: any) => fnSerial('ST_ReadOsm', args),\n    ST_ReadSHP: (...args: any) => fnSerial('ST_ReadSHP', args),\n} as unknown as Fnx\n\n\n\nexport const read_csv = Fncx.read_csv\nexport const read_json = Fncx.read_json\nexport const read_json_objects = Fncx.read_json_objects\nexport const read_parquet = Fncx.read_parquet\nexport const delta_scan = Fncx.delta_scan\nexport const parquet_scan = Fncx.parquet_scan\nexport const read_xlsx = Fncx.read_xlsx\nexport const read_text = Fncx.read_text\nexport const iceberg_metadata = Fncx.iceberg_metadata\nexport const iceberg_scan = Fncx.iceberg_scan\nexport const iceberg_snapshots = Fncx.iceberg_snapshots\nexport const ST_Read = Fncx.ST_Read\nexport const ST_Read_Meta = Fncx.ST_Read_Meta\nexport const ST_ReadOsm = Fncx.ST_ReadOsm\nexport const ST_ReadSHP = Fncx.ST_ReadSHP\n";export{e as default};
